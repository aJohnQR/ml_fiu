{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.special import entr\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed functions for processing the data.\n",
    "\n",
    "# Function to clean text data.\n",
    "def clean_review(text):\n",
    "    # Strip HTML tags\n",
    "    text = re.sub('<[^<]+?>', ' ', text)\n",
    " \n",
    "    # Strip escaped quotes\n",
    "    text = text.replace('\\\\\"', '')\n",
    " \n",
    "    # Strip quotes\n",
    "    text = text.replace('\"', '')\n",
    "    \n",
    "    # Strip @\n",
    "    text = text.replace('@', '')\n",
    " \n",
    "    return text\n",
    "\n",
    "# Puts words into a onehot embedding for the ML models ... I think.\n",
    "def to_sequence(tokenizer, preprocessor, index, text):\n",
    "    words = tokenizer(preprocessor(text))\n",
    "    indexes = [index[word] for word in words if word in index]\n",
    "    return indexes\n",
    "\n",
    "# Shuffle the data function.\n",
    "def shuffle(X, y):\n",
    "    perm = np.random.permutation(len(X))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing for news dataset.\n",
    "# News cell 1/4\n",
    "\n",
    "# This is the data set that throws the error below.\n",
    "# 'utf-8' codec can't decode byte 0x96 in position 37: invalid start byte\n",
    "# need the latin-1 encoding argument in the pd.read_csv line.\n",
    "# The allImdb and tweets dataset seem not to need any modification.\n",
    "\n",
    "data_news = []\n",
    "with open('world news in month_cleaned_2_columsw.csv', 'rt', encoding='latin-1', newline='') as src:\n",
    "    reader_news = csv.reader(src, dialect='excel', lineterminator='\\n')\n",
    "    for row in reader_news:\n",
    "        data_news.extend(row)\n",
    "\n",
    "# Start index at 2 to skip over labels.\n",
    "data_sentiment_news = []\n",
    "data_sentence_news = []\n",
    "for i in range(2, len(data_news)):\n",
    "    if (i % 2) == 0:\n",
    "        data_sentiment_news.extend(data_news[i])\n",
    "    else:\n",
    "        data_sentence_news.extend([data_news[i]])\n",
    "\n",
    "for row in range(0, len(data_sentence_news)):\n",
    "    data_sentence_news[row] = clean_review(data_sentence_news[row])\n",
    "    \n",
    "# Test form of the data.\n",
    "print(data_sentiment_news[1], data_sentence_news[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Cell 2/4\n",
    "\n",
    "# Analysis of news (of cleaned data).\n",
    "global_sentence_news = []\n",
    "unique_word_count_news = 0\n",
    "word_count_news = 0\n",
    "sentence_lengths_array_news = []\n",
    "\n",
    "for i in range(0, len(data_sentence_news)):\n",
    "    sentence = list(data_sentence_news[i].split())\n",
    "    word_count_news += len(sentence)\n",
    "    global_sentence_news.extend(sentence)\n",
    "    sentence_lengths_array_news.extend([len(sentence)])\n",
    "\n",
    "# Some dataset statistics.\n",
    "\n",
    "total_sentences_news = len(data_sentence_news)\n",
    "print('Total number of sentences = ', total_sentences_news)\n",
    "\n",
    "# To get unique word count use set theory.\n",
    "unique_word_count_news = len(set(global_sentence_news))\n",
    "print('Unique word count news = ', unique_word_count_news)\n",
    "\n",
    "# To get total counts.\n",
    "print('Total word count = ', word_count_news)\n",
    "\n",
    "# Some ratios calculated from the data.\n",
    "print('Unique to total word count of news metric = ', unique_word_count_news / word_count_news)\n",
    "print('Unique word to number of sentences news metric = ', unique_word_count_news / total_sentences_news)\n",
    "\n",
    "# Fit a normal distribution to the data:\n",
    "mu_news, std_news = norm.fit(sentence_lengths_array_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Cell 3/4\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(sentence_lengths_array_news, 50, normed=True, color='r')\n",
    "\n",
    "# Plot the PDF.\n",
    "# Note there are two distributions fit to the data. The first is a Gaussian and\n",
    "# the second in non-parametric formed by overlapping kernels.\n",
    "\n",
    "# xmin, xmax = plt.xlim()\n",
    "# x = np.linspace(xmin, xmax, 100)\n",
    "# y = norm.pdf(x, mu_news, std_news)\n",
    "# plt.plot(x, y)\n",
    "plt.xlabel('Length of news samples')\n",
    "plt.ylabel('Number of news samples')\n",
    "plt.title('Sample Length distribution with a fitted distribution')\n",
    "plt.show()\n",
    "sns.distplot([sentence_lengths_array_news])  # seaborn plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Cell 4/4\n",
    "\n",
    "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(data_sentence_news, data_sentiment_news, test_size=0.2)\n",
    "\n",
    "X_train_sequences_news = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_train_news]\n",
    "\n",
    "MAX_SEQ_LENGHT_news = len(max(X_train_sequences_news, key=len))\n",
    "print(\"MAX_SEQ_LENGHT_news=\", MAX_SEQ_LENGHT_news)\n",
    " \n",
    "X_train_sequences_news = pad_sequences(X_train_sequences_news, maxlen=MAX_SEQ_LENGHT_allImdb, value=N_FEATURES)\n",
    "X_test_sequences_news = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_test_news]\n",
    "X_test_sequences_news = pad_sequences(X_test_sequences_news, maxlen=MAX_SEQ_LENGHT_allImdb, value=N_FEATURES)\n",
    "\n",
    "# Need to turn string values into integers for the model.\n",
    "y_train_news = [int(i) for i in y_train_news]\n",
    "y_test_news = [int(i) for i in y_test_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/AJApple/Dropbox/machine-learning-fiu/ml_fiu\n"
     ]
    }
   ],
   "source": [
    "cd ml_fiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml_fiu.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
