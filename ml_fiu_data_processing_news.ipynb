{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.special import entr\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed functions for processing the data.\n",
    "\n",
    "# Function to clean text data.\n",
    "def clean_review(text):\n",
    "    # Strip HTML tags\n",
    "    text = re.sub('<[^<]+?>', ' ', text)\n",
    " \n",
    "    # Strip escaped quotes\n",
    "    text = text.replace('\\\\\"', '')\n",
    " \n",
    "    # Strip quotes\n",
    "    text = text.replace('\"', '')\n",
    "    \n",
    "    # Strip @\n",
    "    text = text.replace('@', '')\n",
    " \n",
    "    return text\n",
    "\n",
    "# Puts words into a onehot embedding for the ML models ... I think.\n",
    "def to_sequence(tokenizer, preprocessor, index, text):\n",
    "    words = tokenizer(preprocessor(text))\n",
    "    indexes = [index[word] for word in words if word in index]\n",
    "    return indexes\n",
    "\n",
    "# Shuffle the data function.\n",
    "def shuffle(X, y):\n",
    "    perm = np.random.permutation(len(X))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing for news dataset.\n",
    "# News cell 1/4\n",
    "\n",
    "# This is the data set that throws the error below.\n",
    "# 'utf-8' codec can't decode byte 0x96 in position 37: invalid start byte\n",
    "# need the latin-1 encoding argument in the pd.read_csv line.\n",
    "# The allImdb and tweets dataset seem not to need any modification.\n",
    "\n",
    "path = '/Users/AJApple/Downloads/'\n",
    "data_news = []\n",
    "with open(path + 'world_news_in_month_cleaned_2_columsw.csv', 'rt', encoding='latin-1', newline='') as src:\n",
    "    reader_news = csv.reader(src, dialect='excel', lineterminator='\\n')\n",
    "    for row in reader_news:\n",
    "        data_news.extend(row)\n",
    "\n",
    "# Start index at 2 to skip over labels.\n",
    "data_sentence_news = []\n",
    "data_sentiment_news = []\n",
    "\n",
    "index = 0\n",
    "for i in range(0, len(data_news)):\n",
    "    if (i % 2) == 0:\n",
    "        data_sentence_news.append(data_news[index])\n",
    "    else:\n",
    "        data_sentiment_news.append([data_news[index]])\n",
    "    index += 1\n",
    "\n",
    "news_sentiment = data_sentiment_news\n",
    "news_text = []\n",
    "for row in range(0, len(data_sentence_news)):\n",
    "    news_text.append(clean_review(data_sentence_news[row]))\n",
    "    \n",
    "# From this cell there are two out put vectors one for the input text called:\n",
    "#     news_text\n",
    "# and one for the sentiment i.e. label/output called:\n",
    "#     news_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US health regulators have approved the first new type of flu drug in two decades. Wednesday's approval of Xofluza for people ages 12 and older comes ahead ...\n",
      "['1']\n",
      "US health regulators have approved the first new type of flu drug in two decades. Wednesday's approval of Xofluza for people ages 12 and older comes ahead ...\n",
      "['1']\n"
     ]
    }
   ],
   "source": [
    "# Test form of the data.\n",
    "\n",
    "a = 52428  # check with random index values\n",
    "print(data_sentence_news[a])\n",
    "print(data_sentiment_news[a])\n",
    "print(news_text[a])\n",
    "print(news_sentiment[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Cell 2/4\n",
    "\n",
    "# Analysis of news (of cleaned data).\n",
    "global_sentence_news = []\n",
    "unique_word_count_news = 0\n",
    "word_count_news = 0\n",
    "sentence_lengths_array_news = []\n",
    "\n",
    "for i in range(0, len(data_sentence_news)):\n",
    "    sentence = list(data_sentence_news[i].split())\n",
    "    word_count_news += len(sentence)\n",
    "    global_sentence_news.extend(sentence)\n",
    "    sentence_lengths_array_news.extend([len(sentence)])\n",
    "\n",
    "# Some dataset statistics.\n",
    "\n",
    "total_sentences_news = len(data_sentence_news)\n",
    "print('Total number of sentences = ', total_sentences_news)\n",
    "\n",
    "# To get unique word count use set theory.\n",
    "unique_word_count_news = len(set(global_sentence_news))\n",
    "print('Unique word count news = ', unique_word_count_news)\n",
    "\n",
    "# To get total counts.\n",
    "print('Total word count = ', word_count_news)\n",
    "\n",
    "# Some ratios calculated from the data.\n",
    "print('Unique to total word count of news metric = ', unique_word_count_news / word_count_news)\n",
    "print('Unique word to number of sentences news metric = ', unique_word_count_news / total_sentences_news)\n",
    "\n",
    "# Fit a normal distribution to the data:\n",
    "mu_news, std_news = norm.fit(sentence_lengths_array_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Cell 3/4\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(sentence_lengths_array_news, 50, normed=True, color='r')\n",
    "\n",
    "# Plot the PDF.\n",
    "# Note there are two distributions fit to the data. The first is a Gaussian and\n",
    "# the second in non-parametric formed by overlapping kernels.\n",
    "\n",
    "# xmin, xmax = plt.xlim()\n",
    "# x = np.linspace(xmin, xmax, 100)\n",
    "# y = norm.pdf(x, mu_news, std_news)\n",
    "# plt.plot(x, y)\n",
    "plt.xlabel('Length of news samples')\n",
    "plt.ylabel('Number of news samples')\n",
    "plt.title('Sample Length distribution with a fitted distribution')\n",
    "plt.show()\n",
    "sns.distplot([sentence_lengths_array_news])  # seaborn plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Cell 4/4\n",
    "\n",
    "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(data_sentence_news, data_sentiment_news, test_size=0.2)\n",
    "\n",
    "X_train_sequences_news = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_train_news]\n",
    "\n",
    "MAX_SEQ_LENGHT_news = len(max(X_train_sequences_news, key=len))\n",
    "print(\"MAX_SEQ_LENGHT_news=\", MAX_SEQ_LENGHT_news)\n",
    " \n",
    "X_train_sequences_news = pad_sequences(X_train_sequences_news, maxlen=MAX_SEQ_LENGHT_allImdb, value=N_FEATURES)\n",
    "X_test_sequences_news = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_test_news]\n",
    "X_test_sequences_news = pad_sequences(X_test_sequences_news, maxlen=MAX_SEQ_LENGHT_allImdb, value=N_FEATURES)\n",
    "\n",
    "# Need to turn string values into integers for the model.\n",
    "y_train_news = [int(i) for i in y_train_news]\n",
    "y_test_news = [int(i) for i in y_test_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing for allImdb dataset.\n",
    "# allImdb Cell 1/4\n",
    "\n",
    "path = '/Users/AJApple/Downloads/'\n",
    "data_allImdb = []\n",
    "reader_allImdb = csv.reader(open(path + 'IMDB_dataset_2.csv'), delimiter=',')\n",
    "for row in reader_allImdb:\n",
    "    data_allImdb.extend(row)\n",
    "\n",
    "# Create empty lists to hold the sentences and sentiment data.\n",
    "# These lists contain all the data. The data will still need to get shuffled and split\n",
    "# into traing and test.\n",
    "data_sentence_allImdb = []\n",
    "data_sentiment_allImdb = []\n",
    "\n",
    "index = 0\n",
    "for i in range(0, len(data_allImdb)):\n",
    "    if (i % 2) == 0:\n",
    "        data_sentence_allImdb.append(data_allImdb[index])\n",
    "    else:\n",
    "        data_sentiment_allImdb.append([data_allImdb[index]])\n",
    "    index += 1\n",
    "\n",
    "imdb_sentiment = []\n",
    "for i in range(0, len(data_sentiment_allImdb)):\n",
    "    if data_sentiment_allImdb[i] == ['positive']:\n",
    "        imdb_sentiment.append(1)\n",
    "    else:\n",
    "        imdb_sentiment.append(0)\n",
    "\n",
    "imdb_text = []\n",
    "# Clean the sentence data i.e. removing html tags, ... etc.\n",
    "for row in range(0, len(data_sentence_allImdb)):\n",
    "    imdb_text.append(clean_review(data_sentence_allImdb[row]))\n",
    "    \n",
    "# From this cell there are two out put vectors one for the input text called:\n",
    "#     imdb_text\n",
    "# and one for the sentiment i.e. label/output called:\n",
    "#     imdb_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got this one a few weeks ago and love it! It's modern, light but filled with true complexities of life. It questions and answers, just like other Eytan Fox movies. This is my favorite, along with Jossi & Jagger. This pictures a lot more, universally, than only the bubbles we may live in. You don't need to be Jewish or homosexual to enjoy this - I'm not, but the movie goes directly to my top ten movies. At first it seems like pure entertainment but it does make you think further. Relationships we have to live with are superficial, meaningful, deep, fatal, you name it. You don't know what's coming, and you definitely don't know where this story is heading as you watch it the first time. It is worth seeing several times. Fox movies include great bonus material - here a great music video and \"the making of\" (including explanation of the title, interviewing Lior Ashknenazi who plays himself in the movie and Arabs with doubts about the Israeli life styles).\n",
      "['positive']\n",
      "I got this one a few weeks ago and love it! It's modern, light but filled with true complexities of life. It questions and answers, just like other Eytan Fox movies. This is my favorite, along with Jossi & Jagger. This pictures a lot more, universally, than only the bubbles we may live in. You don't need to be Jewish or homosexual to enjoy this - I'm not, but the movie goes directly to my top ten movies. At first it seems like pure entertainment but it does make you think further. Relationships we have to live with are superficial, meaningful, deep, fatal, you name it. You don't know what's coming, and you definitely don't know where this story is heading as you watch it the first time. It is worth seeing several times. Fox movies include great bonus material - here a great music video and the making of (including explanation of the title, interviewing Lior Ashknenazi who plays himself in the movie and Arabs with doubts about the Israeli life styles).\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Sanity check cell to make sure data is correct. This should be checked with the \n",
    "# origional csv file data. (It checks out.)\n",
    "\n",
    "a = 49989  # check with random index values\n",
    "print(data_sentence_allImdb[a])\n",
    "print(data_sentiment_allImdb[a])\n",
    "print(imdb_text[a])\n",
    "print(imdb_sentiment[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get data use csv reader cause pandas not working properly.\n",
    "# tweets Cell 1/4\n",
    "\n",
    "path = '/Users/AJApple/Downloads/tweet-sentiment-extraction/'\n",
    "data_tweets = []\n",
    "reader = csv.reader(open(path + 'train_2_columns.csv'), delimiter=',')\n",
    "for row in reader:\n",
    "    data_tweets.extend(row)\n",
    "\n",
    "data_sentence_tweets = []\n",
    "data_sentiment_tweets = []\n",
    "\n",
    "index = 0\n",
    "for i in range(0, len(data_tweets)):\n",
    "    if (i % 2) == 0:\n",
    "        data_sentence_tweets.append(data_tweets[index])\n",
    "    else:\n",
    "        data_sentiment_tweets.append([data_tweets[index]])\n",
    "    index += 1\n",
    "\n",
    "tweets_sentiment = []\n",
    "for i in range(0, len(data_sentiment_tweets)):\n",
    "    if data_sentiment_tweets[i] == ['positive']:\n",
    "        tweets_sentiment.append(1)\n",
    "    elif data_sentiment_tweets[i] == ['negative']:\n",
    "        tweets_sentiment.append(0)\n",
    "    else:\n",
    "        tweets_sentiment.append(2)\n",
    "        \n",
    "tweets_text = []\n",
    "for row in range(0, len(data_sentence_tweets)):\n",
    "    tweets_text.append(clean_review(data_sentence_tweets[row]))\n",
    "    \n",
    "\n",
    "# Now need to remove all the neutral elements\n",
    "\n",
    "index = 0\n",
    "while True:\n",
    "    if tweets_sentiment[index] == 2:\n",
    "        tweets_text.remove(tweets_text[index])\n",
    "        tweets_sentiment.remove(tweets_sentiment[index])\n",
    "        index -= 1\n",
    "    index += 1\n",
    "    if index == len(tweets_text):\n",
    "        break\n",
    "        \n",
    "# From this cell there are two out put vectors one for the input text called:\n",
    "#     tweets_text\n",
    "# and one for the sentiment i.e. label/output called:\n",
    "#     tweets_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " But it was worth it  ****.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Sanity check cell to make sure data is correct. This should be checked with the \n",
    "# origional csv file data. (It checks out.)\n",
    "\n",
    "a = -1\n",
    "print(tweets_text[a])\n",
    "print(tweets_sentiment[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/AJApple/Dropbox/machine-learning-fiu/ml_fiu\n"
     ]
    }
   ],
   "source": [
    "cd /Users/AJApple/Dropbox/machine-learning-fiu/ml_fiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/AJApple/Dropbox/machine-learning-fiu/ml_fiu'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
